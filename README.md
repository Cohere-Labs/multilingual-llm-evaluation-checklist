# Overview

This repository contains a **multilingual evaluation checklist**, which may assist researchers in evaluating generative abilities of their multilingual LLMs. 
It can serve as a guideline when making decisions about which evaluations to report and how.

For context, read our paper ["Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation
"](https://arxiv.org/abs/2504.11829) on best practices for multilingual generative evaluation.
